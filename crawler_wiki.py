import pymongo
import mysql.connector
import wikipediaapi
import time
import os
import logging
from tqdm import tqdm
from dotenv import load_dotenv

load_dotenv()
logging.basicConfig(level=logging.INFO)

# C·∫•u h√¨nh Mongo
MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017/")
mongo_client = pymongo.MongoClient(MONGO_URI)
db_mongo = mongo_client["snake_raw_data"]
col_wiki = db_mongo["wiki_articles"]

# C·∫•u h√¨nh Wiki
wiki_vi = wikipediaapi.Wikipedia(user_agent='SnakeBot/1.0', language='vi')
wiki_en = wikipediaapi.Wikipedia(user_agent='SnakeBot/1.0', language='en')

# --- H√ÄM K·∫æT N·ªêI MYSQL (C·ª¶A B·∫†N) ---
def get_mysql_connection():
    return mysql.connector.connect(
        host=os.getenv("MYSQL_HOST", "localhost"),
        port=os.getenv("MYSQL_PORT", "3306"),
        user=os.getenv("MYSQL_USER", "root"),
        password=os.getenv("MYSQL_PASSWORD", ""),
        database=os.getenv("MYSQL_DB", "snake_db")
    )

def get_snake_names():
    """L·∫•y danh s√°ch t√™n khoa h·ªçc t·ª´ MySQL"""
    names = []
    try:
        conn = get_mysql_connection()
        cursor = conn.cursor()
        logging.info("Fetching species list from MySQL...")
        # L·∫•y genus v√† species ƒë·ªÉ gh√©p l·∫°i
        cursor.execute("SELECT DISTINCT genus, species FROM tax__subspecies")
        rows = cursor.fetchall()
        
        for r in rows:
            names.append(f"{r[0]} {r[1]}")
            
        conn.close()
        logging.info(f"Found {len(names)} species in DB.")
        return names
    except Exception as e:
        logging.error(f"‚ùå Error connecting to MySQL: {e}")
        return []

def run_crawler():
    snakes = get_snake_names()
    if not snakes:
        return

    logging.info(f"üöÄ Starting Wiki Crawler for {len(snakes)} species...")

    for name in tqdm(snakes):
        # 1. Ki·ªÉm tra xem ƒë√£ c√†o ch∆∞a (Tr√°nh spam)
        if col_wiki.find_one({"scientific_name": name}):
            continue

        # 2. Th·ª≠ Wiki Ti·∫øng Vi·ªát tr∆∞·ªõc
        page = wiki_vi.page(name)
        lang = 'vi'
        
        # 3. N·∫øu kh√¥ng c√≥, th·ª≠ Ti·∫øng Anh
        if not page.exists():
            page = wiki_en.page(name)
            lang = 'en'

        # 4. L∆∞u d·ªØ li·ªáu
        if page.exists():
            # L·∫•y text t·ª´ng m·ª•c (sections)
            sections_data = {s.title: s.text[0:1500] for s in page.sections}
            
            data = {
                "scientific_name": name,
                "language": lang,
                "url": page.fullurl,
                "summary": page.summary[0:2000],
                "full_text": page.text[0:5000], # Gi·ªõi h·∫°n 5000 k√Ω t·ª± raw
                "sections": sections_data,
                "processed": False, # Flag ƒë·ªÉ AI x·ª≠ l√Ω sau
                "last_scraped": time.time()
            }
            col_wiki.insert_one(data)
        else:
            # Ghi nh·∫≠n l√† kh√¥ng t√¨m th·∫•y
            col_wiki.insert_one({"scientific_name": name, "found": False})
        
        # 5. Rate Limiting (Quan tr·ªçng)
        time.sleep(1)

    logging.info("‚úÖ Crawler Finished.")

if __name__ == "__main__":
    run_crawler()