import os
import logging
import pandas as pd
import torch
import mysql.connector
from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from tqdm import tqdm

# --- 1. C·∫§U H√åNH ---
load_dotenv()
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)

# K·∫øt n·ªëi Elasticsearch
es = Elasticsearch(
    os.getenv("ES_HOST", "http://localhost:9200"),
    verify_certs=False, 
    ssl_show_warn=False
)

# K·∫øt n·ªëi MySQL
def get_mysql_connection():
    return mysql.connector.connect(
        host=os.getenv("MYSQL_HOST", "localhost"),
        port=os.getenv("MYSQL_PORT", "3306"),
        user=os.getenv("MYSQL_USER", "root"),
        password=os.getenv("MYSQL_PASSWORD", ""),
        database=os.getenv("MYSQL_DB", "snake_db")
    )

# --- C·∫§U H√åNH AI MODEL (BAAI) ---
# BAAI/bge-m3 h·ªó tr·ª£ ƒëa ng√¥n ng·ªØ c·ª±c t·ªët, kh√¥ng c·∫ßn d·ªãch query
MODEL_NAME = 'BAAI/bge-m3'
EMBEDDING_DIMS = 1024  # K√≠ch th∆∞·ªõc vector c·ªßa BGE-M3 l√† 1024

device = 'cuda' if torch.cuda.is_available() else 'cpu'
logging.info(f"üîÑ ƒêang t·∫£i Model AI: {MODEL_NAME} tr√™n thi·∫øt b·ªã: {device}...")
try:
    model = SentenceTransformer(MODEL_NAME, device=device)
    logging.info("‚úÖ T·∫£i Model th√†nh c√¥ng!")
except Exception as e:
    logging.error(f"‚ùå Kh√¥ng t·∫£i ƒë∆∞·ª£c model. L·ªói: {e}")
    exit(1)

# --- 2. H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU ---

def create_index_if_not_exists(index_name="snakes"):
    """T·ª± ƒë·ªông x√≥a v√† t·∫°o l·∫°i Index n·∫øu k√≠ch th∆∞·ªõc Vector thay ƒë·ªïi"""
    if es.indices.exists(index=index_name):
        # Ki·ªÉm tra xem index c≈© c√≥ ƒë√∫ng k√≠ch th∆∞·ªõc 1024 kh√¥ng
        try:
            mapping = es.indices.get_mapping(index=index_name)
            props = mapping[index_name]['mappings']['properties']
            current_dims = props.get('vector_embedding', {}).get('dims', 0)
            
            if current_dims != EMBEDDING_DIMS:
                logging.warning(f"‚ö†Ô∏è Index c≈© ({current_dims} dims) kh√¥ng kh·ªõp model m·ªõi ({EMBEDDING_DIMS} dims).")
                logging.warning("üóëÔ∏è ƒêang x√≥a Index c≈© ƒë·ªÉ t·∫°o l·∫°i...")
                es.indices.delete(index=index_name)
            else:
                logging.info(f"‚úÖ Index '{index_name}' ƒë√£ t·ªìn t·∫°i v√† ƒë√∫ng c·∫•u h√¨nh.")
                return
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Kh√¥ng ki·ªÉm tra ƒë∆∞·ª£c mapping c≈©, s·∫Ω t·∫°o l·∫°i. L·ªói: {e}")

    # T·∫°o Mapping m·ªõi
    mapping = {
        "mappings": {
            "properties": {
                "id": {"type": "keyword"},
                "scientific_name": {"type": "text"},
                "common_names": {"type": "text", "analyzer": "standard"},
                "family": {"type": "keyword"},
                "danger_level": {"type": "keyword"},
                "max_len_cm": {"type": "float"}, # ƒê·ªïi sang float ƒë·ªÉ sort n·∫øu c·∫ßn
                "full_text_context": {"type": "text"},
                "vector_embedding": {
                    "type": "dense_vector",
                    "dims": EMBEDDING_DIMS,
                    "index": True,
                    "similarity": "cosine" # Cosine t·ªët cho ng·ªØ nghƒ©a
                }
            }
        }
    }
    es.indices.create(index=index_name, body=mapping)
    logging.info(f"‚úÖ ƒê√£ t·∫°o Index '{index_name}' v·ªõi k√≠ch th∆∞·ªõc Vector: {EMBEDDING_DIMS}")

def fetch_snake_data():
    """L·∫•y d·ªØ li·ªáu t·ª´ MySQL"""
    conn = get_mysql_connection()
    cursor = conn.cursor(dictionary=True)
    
    logging.info("üì• ƒêang l·∫•y d·ªØ li·ªáu t·ª´ MySQL...")
    
    query = """
    SELECT 
        TRIM(CONCAT(t.genus, ' ', t.species, ' ', t.subspecies)) AS full_scientific_name,
        tf.family,
        (SELECT GROUP_CONCAT(DISTINCT cname SEPARATOR ', ') FROM map__cname m WHERE m.genus = t.genus AND m.species = t.species AND m.subspecies = t.subspecies) AS common_names,
        (SELECT danger FROM map__danger d WHERE d.genus = t.genus AND d.species = t.species AND d.subspecies = t.subspecies LIMIT 1) AS danger_level,
        (SELECT MAX(tbl) FROM val__size s WHERE s.genus = t.genus AND s.species = t.species AND s.subspecies = t.subspecies) AS max_len_cm,
        (SELECT reproduction FROM map__reproduction r WHERE r.genus = t.genus AND r.species = t.species AND r.subspecies = t.subspecies LIMIT 1) AS reproduction
    FROM tax__subspecies t
    LEFT JOIN tax__genus tg ON t.genus = tg.genus
    LEFT JOIN tax__family tf ON tg.family = tf.family
    """
    
    try:
        cursor.execute(query)
        df = pd.DataFrame(cursor.fetchall())
        logging.info(f"üìä ƒê√£ l·∫•y {len(df)} d√≤ng d·ªØ li·ªáu.")
        return df
    except Exception as e:
        logging.error(f"‚ùå L·ªói SQL: {e}")
        return pd.DataFrame()
    finally:
        cursor.close()
        conn.close()

def construct_context(row):
    """T·∫°o ƒëo·∫°n vƒÉn m√¥ t·∫£ ƒë·∫ßy ƒë·ªß ƒë·ªÉ AI Embed"""
    # X·ª≠ l√Ω Null
    cnames = row['common_names'] if row['common_names'] else "Unknown"
    danger = row['danger_level'] if row['danger_level'] else "Unknown"
    family = row['family'] if row['family'] else "Unknown"
    
    # Text n√†y s·∫Ω ƒë∆∞·ª£c bi·∫øn th√†nh Vector. C√†ng chi ti·∫øt c√†ng t·ªët.
    # Model BAAI hi·ªÉu c·∫£ Anh l·∫´n Vi·ªát, nh∆∞ng d·ªØ li·ªáu g·ªëc n√™n ƒë·ªÉ ti·∫øng Anh chu·∫©n khoa h·ªçc.
    text = (
        f"Species: {row['full_scientific_name']}. "
        f"Common names: {cnames}. "
        f"Family: {family}. "
        f"Danger level: {danger}. "
        f"Max length: {row['max_len_cm']} cm. "
        f"Reproduction: {row['reproduction']}."
    )
    return text

def run_etl(batch_size=64):
    create_index_if_not_exists("snakes")
    df = fetch_snake_data()
    if df.empty: return

    logging.info("üìù ƒêang t·∫°o ng·ªØ c·∫£nh (Context building)...")
    df['full_text_context'] = df.apply(construct_context, axis=1)
    
    logging.info(f"üöÄ B·∫Øt ƒë·∫ßu Embed v√† Index {len(df)} b·∫£n ghi...")
    
    # X·ª≠ l√Ω theo batch ƒë·ªÉ tr√°nh tr√†n RAM
    for i in tqdm(range(0, len(df), batch_size), desc="Indexing"):
        batch = df.iloc[i : i + batch_size].copy()
        
        try:
            # Encode b·∫±ng BAAI/bge-m3
            embeddings = model.encode(batch['full_text_context'].tolist(), show_progress_bar=False)
            batch['vector_embedding'] = embeddings.tolist()
            
            actions = []
            for rec in batch.to_dict(orient="records"):
                doc_id = rec['full_scientific_name'].replace(" ", "_")
                action = {
                    "_index": "snakes",
                    "_id": doc_id,
                    "_source": {
                        "id": doc_id,
                        "scientific_name": rec['full_scientific_name'],
                        "common_names": rec.get('common_names', ''),
                        "family": rec.get('family', ''),
                        "danger_level": rec.get('danger_level', ''),
                        "max_len_cm": rec.get('max_len_cm'),
                        "full_text_context": rec['full_text_context'],
                        "vector_embedding": rec['vector_embedding']
                    }
                }
                actions.append(action)
            
            if actions:
                helpers.bulk(es, actions)
                
        except Exception as e:
            logging.error(f"‚ùå L·ªói t·∫°i batch {i}: {e}")

    logging.info("üéâ ETL Ho√†n t·∫•t! D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng.")

if __name__ == "__main__":
    if es.ping():
        run_etl()
    else:
        logging.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi Elasticsearch.")