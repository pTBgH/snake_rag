import os
import pymongo
from tqdm import tqdm
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from dotenv import load_dotenv

load_dotenv()

MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017/")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
MODEL_NAME = "google/gemini-2.5-flash-lite" 

client = pymongo.MongoClient(MONGO_URI)
db = client["snake_raw_data"]
col_raw = db["wiki_articles"]
col_clean = db["wiki_cleaned"]

# --- [C·∫¨P NH·∫¨T] TH√äM TR∆Ø·ªúNG vietnamese_name ---
class SnakeDataClean(BaseModel):
    vietnamese_name: str = Field(description="T√™n th∆∞·ªùng g·ªçi ch√≠nh x√°c b·∫±ng Ti·∫øng Vi·ªát c·ªßa lo√†i n√†y (VD: R·∫Øn h·ªï mang ch√∫a). N·∫øu kh√¥ng c√≥, tr·∫£ v·ªÅ chu·ªói r·ªóng.")
    biology: str = Field(description="M√¥ t·∫£ h√¨nh d√°ng, m√†u s·∫Øc (Ti·∫øng Vi·ªát).")
    venom: str = Field(description="Th√¥ng tin n·ªçc ƒë·ªôc. N·∫øu kh√¥ng ƒë·ªôc ghi 'Kh√¥ng ƒë·ªôc'.")
    behavior: str = Field(description="T·∫≠p t√≠nh, sinh s·∫£n.")
    distribution: str = Field(description="Khu v·ª±c ph√¢n b·ªë.")

llm = ChatOpenAI(
    openai_api_key=OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1",
    model=MODEL_NAME,
    temperature=0
)

parser = JsonOutputParser(pydantic_object=SnakeDataClean)

prompt = ChatPromptTemplate.from_template("""
B·∫°n l√† chuy√™n gia sinh h·ªçc. ƒê·ªçc vƒÉn b·∫£n v√† tr√≠ch xu·∫•t th√¥ng tin JSON.
Quan tr·ªçng: H√£y t√¨m t√™n g·ªçi ph·ªï bi·∫øn nh·∫•t b·∫±ng Ti·∫øng Vi·ªát c·ªßa lo√†i n√†y trong vƒÉn b·∫£n.

VƒÇN B·∫¢N:
{text}

JSON OUTPUT:
{format_instructions}
""")

chain = prompt | llm | parser

def run_ai_processing():
    # L·∫•y c√°c b√†i ch∆∞a x·ª≠ l√Ω AI (ho·∫∑c b·∫°n c√≥ th·ªÉ x√≥a col_clean ƒëi ch·∫°y l·∫°i t·ª´ ƒë·∫ßu)
    query = {"processed": False, "found": {"$ne": False}}
    total = col_raw.count_documents(query)
    cursor = col_raw.find(query)
    
    print(f"ü§ñ B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t t√™n Ti·∫øng Vi·ªát cho {total} lo√†i...")

    for doc in tqdm(cursor, total=total):
        try:
            raw_text = f"Summary: {doc.get('summary', '')}\n"
            # Th√™m title v√†o text ƒë·ªÉ AI d·ªÖ b·∫Øt t√™n
            raw_text += f"Title from URL: {doc.get('url', '')}\n" 
            
            cleaned_data = chain.invoke({
                "text": raw_text[:6000],
                "format_instructions": parser.get_format_instructions()
            })

            col_clean.update_one(
                {"scientific_name": doc['scientific_name']},
                {"$set": {
                    "scientific_name": doc['scientific_name'],
                    "ai_data": cleaned_data,
                    "original_url": doc.get('url')
                }},
                upsert=True
            )
            col_raw.update_one({"_id": doc["_id"]}, {"$set": {"processed": True}})

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói: {doc['scientific_name']} - {e}")

if __name__ == "__main__":
    run_ai_processing()