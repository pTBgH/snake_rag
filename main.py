import os
import logging
import time
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from dotenv import load_dotenv

from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- C·∫§U H√åNH ---
load_dotenv()
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# Model LLM (B·∫°n c√≥ th·ªÉ ƒë·ªïi model kh√°c tr√™n OpenRouter t·∫°i ƒë√¢y)
OPENROUTER_MODEL = "google/gemini-2.5-flash-lite"

resources = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    # 1. K·∫øt n·ªëi Elasticsearch
    es_host = os.getenv("ES_HOST", "http://localhost:9200")
    es_client = Elasticsearch(es_host, verify_certs=False, ssl_show_warn=False)
    
    # 2. Load Model Embedding (B·∫ÆT BU·ªòC KH·ªöP V·ªöI ETL)
    model_name = 'BAAI/bge-m3'
    logging.info(f"‚è≥ ƒêang t·∫£i Model Embedding ({model_name})...")
    # T·∫£i model v√†o RAM khi kh·ªüi ƒë·ªông app
    embed_model = SentenceTransformer(model_name)
    logging.info("‚úÖ Embedding Model ƒë√£ s·∫µn s√†ng!")

    # 3. K·∫øt n·ªëi OpenRouter LLM
    llm = ChatOpenAI(
        openai_api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        model=OPENROUTER_MODEL, 
        temperature=0.3, # Nhi·ªát ƒë·ªô th·∫•p ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c, √≠t b·ªãa
        max_retries=1    # Th·ª≠ l·∫°i 1 l·∫ßn th√¥i, kh√¥ng ƒë∆∞·ª£c th√¨ Fallback ngay
    )

    # 4. Prompt Engineering "Kh√¥n ngoan"
    # D√πng BAAI n√™n kh√¥ng c·∫ßn b∆∞·ªõc d·ªãch (Translator), ƒë∆∞a th·∫≥ng ng·ªØ c·∫£nh v√†o Prompt
    answer_prompt = ChatPromptTemplate.from_template("""
    B·∫°n l√† m·ªôt chuy√™n gia b√≤ s√°t h·ªçc (Herpetologist) am hi·ªÉu v√† c·∫©n th·∫≠n.
    Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng HO√ÄN TO√ÄN B·∫∞NG TI·∫æNG VI·ªÜT.
    
    D·ªØ li·ªáu tham kh·∫£o (Context):
    {context}

    C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:
    {question}

    Y√äU C·∫¶U TR·∫¢ L·ªúI:
    1. **An to√†n l√† tr√™n h·∫øt**: N·∫øu Context c√≥ t·ª´ kh√≥a 'venom', 'poison', 'dangerous', h√£y b·∫Øt ƒë·∫ßu c√¢u tr·∫£ l·ªùi b·∫±ng: "‚ö†Ô∏è **C·∫¢NH B√ÅO: LO√ÄI R·∫ÆN N√ÄY C√ì ƒê·ªòC/NGUY HI·ªÇM!**".
    2. **C·∫•u tr√∫c r√µ r√†ng**:
       - T√™n khoa h·ªçc & T√™n th∆∞·ªùng g·ªçi.
       - ƒê·∫∑c ƒëi·ªÉm nh·∫≠n d·∫°ng (K√≠ch th∆∞·ªõc, m√†u s·∫Øc n·∫øu c√≥).
       - M·ª©c ƒë·ªô nguy hi·ªÉm.
       - N∆°i s·ªëng/Sinh s·∫£n (n·∫øu c√≥ trong context).
    3. **D·ªãch thu·∫≠t ng·ªØ**: H√£y d·ªãch c√°c t·ª´ nh∆∞ Family (H·ªç), Danger Level (M·ª©c ƒë·ªô ƒë·ªôc) sang ti·∫øng Vi·ªát.
    4. **Trung th·ª±c**: N·∫øu Context kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i "D·ªØ li·ªáu hi·ªán t·∫°i ch∆∞a c√≥ th√¥ng tin chi ti·∫øt v·ªÅ v·∫•n ƒë·ªÅ b·∫°n h·ªèi."

    H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch v√† chuy√™n nghi·ªáp.
    """)
    
    llm_chain = answer_prompt | llm | StrOutputParser()
    
    resources["es"] = es_client
    resources["embed"] = embed_model
    resources["llm"] = llm_chain
    
    yield
    # D·ªçn d·∫πp t√†i nguy√™n khi t·∫Øt app
    resources.clear()

app = FastAPI(title="Snake RAG API", lifespan=lifespan)

class QueryRequest(BaseModel):
    question: str

# --- H√ÄM FALLBACK (QUAN TR·ªåNG) ---
def format_fallback_response_vn(hits, error_msg=""):
    """
    H√†m n√†y ch·∫°y khi LLM b·ªã l·ªói (m·∫•t m·∫°ng, h·∫øt ti·ªÅn, qu√° t·∫£i).
    N√≥ bi·∫øn d·ªØ li·ªáu th√¥ t·ª´ ES th√†nh c√¢u tr·∫£ l·ªùi Ti·∫øng Vi·ªát d·ªÖ ƒë·ªçc.
    """
    if not hits:
        return "Xin l·ªói, h·ªá th·ªëng kh√¥ng t√¨m th·∫•y lo√†i r·∫Øn n√†o ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu."
    
    # Header th√¥ng b√°o ch·∫ø ƒë·ªô Fallback
    response = f"‚ö†Ô∏è **L∆∞u √Ω**: {error_msg} H·ªá th·ªëng ƒëang hi·ªÉn th·ªã d·ªØ li·ªáu g·ªëc t·ª´ kho l∆∞u tr·ªØ:\n\n"
    
    for i, hit in enumerate(hits, 1):
        src = hit['_source']
        
        # X·ª≠ l√Ω icon c·∫£nh b√°o d·ª±a tr√™n text
        danger_text = str(src.get('danger_level', '')).lower()
        is_dangerous = any(x in danger_text for x in ['venom', 'danger', 'fatal', 'toxic'])
        icon = "‚ò†Ô∏è" if is_dangerous else "üü¢"
        
        # D·ªãch s∆° b·ªô m·ªôt s·ªë tr∆∞·ªùng
        family = src.get('family', 'Kh√¥ng r√µ')
        size = src.get('max_len_cm')
        size_str = f"{size} cm" if size else "Ch∆∞a c√≥ d·ªØ li·ªáu"
        
        response += f"**{i}. {src.get('scientific_name')}**\n"
        response += f"   - **T√™n g·ªçi kh√°c**: {src.get('common_names')}\n"
        response += f"   - **H·ªç**: {family}\n"
        response += f"   - **ƒê·ªô ƒë·ªôc**: {icon} {src.get('danger_level')}\n"
        response += f"   - **K√≠ch th∆∞·ªõc t·ªëi ƒëa**: {size_str}\n"
        # C·∫Øt ng·∫Øn m√¥ t·∫£ ƒë·ªÉ kh√¥ng qu√° d√†i
        desc = src.get('full_text_context', '')[:150] + "..."
        response += f"   - **Th√¥ng tin g·ªëc**: _{desc}_\n\n"
        
    return response

@app.post("/api/ask-snake")
async def ask_snake_endpoint(request: QueryRequest):
    start_time = time.time()
    user_question = request.question.strip()
    
    es_client = resources.get("es")
    embed_model = resources.get("embed")
    llm_qa = resources.get("llm")

    # B∆Ø·ªöC 1: EMBEDDING (Vector h√≥a c√¢u h·ªèi)
    try:
        # BAAI/bge-m3 x·ª≠ l√Ω ti·∫øng Vi·ªát r·∫•t t·ªët, embed tr·ª±c ti·∫øp
        query_vector = embed_model.encode(user_question).tolist()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω AI (Embedding): {str(e)}")

    # B∆Ø·ªöC 2: T√åM KI·∫æM (Elasticsearch)
    try:
        search_body = {
            "size": 3, # L·∫•y 3 k·∫øt qu·∫£ t·ªët nh·∫•t
            "query": {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        # C·ªông 1.0 ƒë·ªÉ ƒë·∫£m b·∫£o ƒëi·ªÉm s·ªë lu√¥n d∆∞∆°ng
                        "source": "cosineSimilarity(params.query_vector, 'vector_embedding') + 1.0",
                        "params": {"query_vector": query_vector}
                    }
                }
            },
            "_source": ["scientific_name", "common_names", "family", "danger_level", "max_len_cm", "full_text_context"]
        }
        response = es_client.search(index="snakes", body=search_body)
        hits = response['hits']['hits']
    except Exception as e:
        # N·∫øu m·∫•t k·∫øt n·ªëi ES th√¨ ch·ªãu, tr·∫£ l·ªói lu√¥n
        raise HTTPException(status_code=500, detail=f"L·ªói k·∫øt n·ªëi CSDL: {str(e)}")

    # L·ªçc k·∫øt qu·∫£ r√°c (ng∆∞·ª°ng score > 1.35 cho BGE-M3 l√† kh√° an to√†n)
    valid_hits = [hit for hit in hits if hit['_score'] > 1.35]
    
    # L·∫•y danh s√°ch t√™n ngu·ªìn
    sources = list(set([h['_source']['scientific_name'] for h in valid_hits]))

    if not valid_hits:
        return {
            "answer": "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†o v·ªÅ lo√†i r·∫Øn n√†y trong h·ªá th·ªëng.",
            "sources": [],
            "mode": "no_result",
            "time_taken": f"{time.time() - start_time:.2f}s"
        }

    # B∆Ø·ªöC 3: SINH C√ÇU TR·∫¢ L·ªúI (Th·ª≠ LLM -> N·∫øu l·ªói -> Fallback)
    try:
        # Chu·∫©n b·ªã context d·∫°ng text cho AI ƒë·ªçc
        context_text = "\n\n".join([
            f"Snake {i+1}: {h['_source']['full_text_context']}" 
            for i, h in enumerate(valid_hits)
        ])
        
        # G·ªçi LLM OpenRouter
        logging.info("ü§ñ ƒêang g·ª≠i request t·ªõi OpenRouter...")
        answer = llm_qa.invoke({
            "context": context_text,
            "question": user_question
        })
        mode = "ai_expert" # Ch·∫ø ƒë·ªô tr·∫£ l·ªùi th√¥ng minh

    except Exception as e:
        logging.error(f"‚ö†Ô∏è LLM Error (OpenRouter/Gemini): {e}")
        # --- K√çCH HO·∫†T FALLBACK ---
        # T·ª± ƒë·ªông chuy·ªÉn sang ch·∫ø ƒë·ªô tr·∫£ d·ªØ li·ªáu th√¥
        answer = format_fallback_response_vn(valid_hits, error_msg="K·∫øt n·ªëi AI ƒëang gi√°n ƒëo·∫°n.")
        mode = "fallback_offline" # Ch·∫ø ƒë·ªô d·ª± ph√≤ng

    return {
        "answer": answer,
        "sources": sources,
        "mode": mode,
        "time_taken": f"{time.time() - start_time:.2f}s"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)